{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "项目根目录设置为: d:\\SCUT\\Second Year\\Second Semester\\DeepLearningProject\\SST_Prediciton\n",
      "原始数据路径: d:\\SCUT\\Second Year\\Second Semester\\DeepLearningProject\\SST_Prediciton\\data/raw/ERA5\n",
      "预处理数据路径: d:\\SCUT\\Second Year\\Second Semester\\DeepLearningProject\\SST_Prediciton\\data/processed/ERA5\n",
      "设备设置为: cuda\n",
      "训练轮数 (NUM_EPOCHS) 设置为: 5\n"
     ]
    }
   ],
   "source": [
    "# 单元格 1: Notebook 设置与全局配置\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd # 用于日期处理和插值\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import glob # 用于查找文件\n",
    "from scipy.interpolate import griddata\n",
    "import math\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 项目根目录设定 ---\n",
    "# 假设此Notebook文件位于项目根目录 \"sst_diffusion_baseline/\" 下\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) # 获取当前Notebook的工作目录 \n",
    "\n",
    "# --- 数据路径 ---\n",
    "# DATA_RAW_PATH = \"/mnt/sata_16t/yaweiwang/ERA5\"\n",
    "DATA_RAW_PATH = os.path.join(PROJECT_ROOT, \"data/raw/ERA5\") # 原始数据目录\n",
    "DATA_PROCESSED_PATH = os.path.join(PROJECT_ROOT, \"data/processed/ERA5\") \n",
    "PATCHES_PATH = os.path.join(DATA_PROCESSED_PATH, \"patches\")\n",
    "LAND_SEA_MASK_PATH = os.path.join(DATA_PROCESSED_PATH, \"land_sea_mask.pt\")\n",
    "NORMALIZATION_STATS_PATH = os.path.join(DATA_PROCESSED_PATH, \"normalization_stats.pt\")\n",
    "\n",
    "# --- 数据参数 ---\n",
    "SST_VARIABLE_NAME = 'sst'\n",
    "LON_VARIABLE_NAME = 'longitude'\n",
    "LAT_VARIABLE_NAME = 'latitude'\n",
    "FILENAME_TEMPLATE = \"{date_str}_ERA5_daily_mean_sst.nc\"\n",
    "\n",
    "IMAGE_TARGET_HEIGHT = 720\n",
    "IMAGE_TARGET_WIDTH = 1440\n",
    "\n",
    "# --- 预处理参数 ---\n",
    "PATCH_SIZE = 64\n",
    "STRIDE = 16\n",
    "\n",
    "DATA_START_DATE = \"2020-01-01\"\n",
    "DATA_END_DATE = \"2020-03-31\"\n",
    "TRAIN_PERIOD_END_DATE = \"2020-02-29\"\n",
    "\n",
    "# --- 特征工程参数 ---\n",
    "MLP_HIDDEN_DIMS = [128] # MLP的隐藏层维度\n",
    "CONDITION_EMBED_DIM = 256 # MLP的输出维度，也是U-Net期望的条件向量维度\n",
    "\n",
    "# --- 模型参数 ---\n",
    "MODEL_ARCHITECTURE = \"classic_unet\"\n",
    "HISTORY_DAYS = 30\n",
    "TARGET_DAYS = 1\n",
    "\n",
    "UNET_IN_CHANNELS = HISTORY_DAYS + TARGET_DAYS\n",
    "UNET_OUT_CHANNELS = TARGET_DAYS\n",
    "UNET_BLOCK_OUT_CHANNELS = (64, 128, 256, 256)\n",
    "UNET_DOWN_BLOCK_TYPES = tuple([\"DownBlock2D\"] * len(UNET_BLOCK_OUT_CHANNELS))\n",
    "UNET_UP_BLOCK_TYPES = tuple([\"UpBlock2D\"] * len(UNET_BLOCK_OUT_CHANNELS))\n",
    "UNET_CLASS_EMBED_TYPE = \"identity\"\n",
    "UNET_NUM_CLASS_EMBEDS  = CONDITION_EMBED_DIM\n",
    "\n",
    "# --- 训练参数 ---\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 5 # 笔记本中测试时可以改小，例如 1-5 轮\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "DDPM_NUM_TRAIN_TIMESTEPS = 1000\n",
    "DDPM_BETA_SCHEDULE = \"linear\"\n",
    "\n",
    "# --- 推理参数 ---\n",
    "DDPM_NUM_INFERENCE_STEPS = 50\n",
    "AUTOREGRESSIVE_PREDICT_DAYS = 3\n",
    "\n",
    "# --- 结果保存路径 ---\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT, \"results\")\n",
    "CHECKPOINT_PATH = os.path.join(RESULTS_PATH, \"checkpoints\")\n",
    "FIGURES_PATH = os.path.join(RESULTS_PATH, \"figures\")\n",
    "PREDICTIONS_PATH = os.path.join(RESULTS_PATH, \"predictions\")\n",
    "\n",
    "\n",
    "# --- 其他 ---\n",
    "RANDOM_SEED = 42\n",
    "LOG_INTERVAL = 1000 # 训练时日志打印间隔\n",
    "SAVE_EPOCH_INTERVAL = 10 # 训练时模型保存间隔 (笔记本中测试时可以改大或不保存)\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(f\"项目根目录设置为: {PROJECT_ROOT}\")\n",
    "print(f\"原始数据路径: {DATA_RAW_PATH}\")\n",
    "print(f\"预处理数据路径: {DATA_PROCESSED_PATH}\")\n",
    "print(f\"设备设置为: {DEVICE}\")\n",
    "print(f\"训练轮数 (NUM_EPOCHS) 设置为: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 单元格 2 演示 ---\n",
      "目录已存在: d:\\SCUT\\Second Year\\Second Semester\\DeepLearningProject\\SST_Prediciton\\data/raw/ERA5\n",
      "目录已存在: d:\\SCUT\\Second Year\\Second Semester\\DeepLearningProject\\SST_Prediciton\\data/processed/ERA5\n",
      "\n",
      "加载示例文件: d:\\SCUT\\Second Year\\Second Semester\\DeepLearningProject\\SST_Prediciton\\data/raw/ERA5\\20200101_ERA5_daily_mean_sst.nc\n",
      "纬度坐标已反转为升序。\n",
      "成功加载原始SST数据，形状: (721, 1440)\n",
      "SST值范围 (原始): 270.19 to 306.92\n",
      "纬度范围: -90.00 to 90.00, 点数: 721\n",
      "经度范围: 0.00 to 359.75, 点数: 1440\n",
      "\n",
      "尝试裁剪图像到 (720, 1440)...\n",
      "原始高度为721，目标为720，将裁剪掉最后一行纬度。\n",
      "裁剪后的SST数据形状: (720, 1440)\n"
     ]
    }
   ],
   "source": [
    "# 单元格 2: 预处理辅助函数与文件加载/裁剪\n",
    "\n",
    "def ensure_dir(directory_path):\n",
    "    \"\"\"确保目录存在，如果不存在则创建\"\"\"\n",
    "    if not os.path.exists(directory_path):\n",
    "        os.makedirs(directory_path)\n",
    "        print(f\"目录已创建: {directory_path}\")\n",
    "    else:\n",
    "        print(f\"目录已存在: {directory_path}\")\n",
    "\n",
    "def load_single_nc_file(file_path):\n",
    "    \"\"\"\n",
    "    加载单个NetCDF文件，并提取SST、经度和纬度数据。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ds = xr.open_dataset(file_path)\n",
    "        sst_data = ds[SST_VARIABLE_NAME] # 使用单元格1定义的全局变量\n",
    "        lon = ds.get(LON_VARIABLE_NAME, ds.get('lon', None))\n",
    "        lat = ds.get(LAT_VARIABLE_NAME, ds.get('lat', None))\n",
    "\n",
    "        if lon is None or lat is None:\n",
    "            raise ValueError(\"NetCDF 文件中未找到经度或纬度变量。\")\n",
    "\n",
    "        # 确保SST数据的维度顺序是 (latitude, longitude)\n",
    "        if sst_data.dims[0] == lon.name and sst_data.dims[1] == lat.name:\n",
    "            sst_data = sst_data.transpose(lat.name, lon.name)\n",
    "        elif sst_data.dims[0] != lat.name or sst_data.dims[1] != lon.name:\n",
    "            if len(sst_data.dims) == 2:\n",
    "                if ds[lat.name].size == sst_data.shape[0] and ds[lon.name].size == sst_data.shape[1]:\n",
    "                    sst_data = sst_data.rename({sst_data.dims[0]: lat.name, sst_data.dims[1]: lon.name})\n",
    "                elif ds[lon.name].size == sst_data.shape[0] and ds[lat.name].size == sst_data.shape[1]:\n",
    "                     sst_data = sst_data.rename({sst_data.dims[0]: lon.name, sst_data.dims[1]: lat.name})\n",
    "                     sst_data = sst_data.transpose(lat.name, lon.name)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"SST数据维度 {sst_data.dims} 与经纬度名称和大小不匹配。\"\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(f\"SST数据具有意外的维度数量: {len(sst_data.dims)}\")\n",
    "\n",
    "        if 'time' in sst_data.dims and len(sst_data.time) == 1:\n",
    "            sst_data = sst_data.squeeze('time')\n",
    "        \n",
    "        # 确保经纬度坐标是升序的 (xarray 的 interpolate_na 可能需要)\n",
    "        if lat.values[0] > lat.values[-1]: # 如果是降序\n",
    "            sst_data = sst_data.reindex({lat.name: sorted(lat.values)})\n",
    "            lat = sst_data[lat.name] # 更新lat坐标的引用\n",
    "            print(f\"纬度坐标已反转为升序。\")\n",
    "            \n",
    "        if lon.values[0] > lon.values[-1]: # 如果是降序 (通常经度是0-360升序)\n",
    "             # 对于经度，如果是0-359然后突然变为负数（如-180到180），处理会更复杂。\n",
    "             # 假设目前是单调的。\n",
    "             pass # 通常经度是升序的\n",
    "\n",
    "        return sst_data, lon, lat\n",
    "    except Exception as e:\n",
    "        print(f\"加载或处理文件 {file_path} 时出错: {e}\")\n",
    "        raise\n",
    "\n",
    "def crop_image(data_array: xr.DataArray, target_height: int, target_width: int) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    裁剪 xarray.DataArray 到目标尺寸。\n",
    "    \"\"\"\n",
    "    current_height, current_width = data_array.shape\n",
    "    \n",
    "\n",
    "    if current_height == 721 and target_height == 720:\n",
    "         # 假设纬度是升序的，去掉第一行（例如南纬90度）或最后一行（例如北纬90度）\n",
    "         # 如果纬度从-90到90，data_array.isel({data_array.dims[0]: slice(1, None)}) 会去掉-90度\n",
    "         # 或 data_array.isel({data_array.dims[0]: slice(0, -1)}) 会去掉+90度\n",
    "         # 这里简单地假设去掉最后一行。如果你的数据纬度是降序，这可能是第一行。\n",
    "         # 确保 load_single_nc_file 中纬度已转为升序，则 data_array[:-1, :] 是去掉最高的纬度\n",
    "         print(f\"原始高度为721，目标为720，将裁剪掉最后一行纬度。\")\n",
    "         data_array = data_array[:-1, :] \n",
    "         current_height = data_array.shape[0]\n",
    "    \n",
    "    if current_height == target_height and current_width == target_width:\n",
    "        return data_array\n",
    "    elif current_height < target_height or current_width < target_width:\n",
    "        raise ValueError(f\"原始图像 ({current_height},{current_width}) 小于目标尺寸 ({target_height},{target_width})。无法裁剪。\")\n",
    "    \n",
    "\n",
    "\n",
    "# --- 演示如何使用这些函数 ---\n",
    "print(\"\\n--- 单元格 2 演示 ---\")\n",
    "ensure_dir(DATA_RAW_PATH) # 确保原始数据目录存在 (虽然这里不创建文件)\n",
    "ensure_dir(DATA_PROCESSED_PATH) # 确保后续保存文件的目录存在\n",
    "\n",
    "# 构造一个示例文件名 (你需要确保这个文件确实在 DATA_RAW_PATH 中)\n",
    "# 我们使用 DATA_START_DATE 来构造第一个文件名\n",
    "try:\n",
    "    example_date_dt = datetime.strptime(DATA_START_DATE, \"%Y-%m-%d\")\n",
    "    example_date_str_nodash = example_date_dt.strftime(\"%Y%m%d\")\n",
    "    example_filename = FILENAME_TEMPLATE.format(date_str=example_date_str_nodash)\n",
    "    example_filepath = os.path.join(DATA_RAW_PATH, example_filename)\n",
    "\n",
    "    if os.path.exists(example_filepath):\n",
    "        print(f\"\\n加载示例文件: {example_filepath}\")\n",
    "        sst_array_raw, lon_coords, lat_coords = load_single_nc_file(example_filepath)\n",
    "        print(f\"成功加载原始SST数据，形状: {sst_array_raw.shape}\")\n",
    "        print(f\"SST值范围 (原始): {float(sst_array_raw.min()):.2f} to {float(sst_array_raw.max()):.2f}\")\n",
    "        print(f\"纬度范围: {float(lat_coords.min()):.2f} to {float(lat_coords.max()):.2f}, 点数: {len(lat_coords)}\")\n",
    "        print(f\"经度范围: {float(lon_coords.min()):.2f} to {float(lon_coords.max()):.2f}, 点数: {len(lon_coords)}\")\n",
    "\n",
    "\n",
    "        print(f\"\\n尝试裁剪图像到 ({IMAGE_TARGET_HEIGHT}, {IMAGE_TARGET_WIDTH})...\")\n",
    "        sst_array_cropped = crop_image(sst_array_raw, IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH)\n",
    "        print(f\"裁剪后的SST数据形状: {sst_array_cropped.shape}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"错误: 示例文件 {example_filepath} 未找到。请确保文件存在或修改 DATA_START_DATE。\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"单元格2演示部分发生错误: {e}\")\n",
    "    # 定义一个空的或假的 sst_array_cropped\n",
    "    sst_array_cropped = xr.DataArray(np.full((IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH), np.nan), \n",
    "                                     dims=('latitude', 'longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'latitude' (latitude: 721)> Size: 3kB\n",
      "array([-90.  , -89.75, -89.5 , ...,  89.5 ,  89.75,  90.  ], dtype=float32)\n",
      "Coordinates:\n",
      "  * latitude  (latitude) float32 3kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "Attributes:\n",
      "    units:      degrees_north\n",
      "    long_name:  latitude\n"
     ]
    }
   ],
   "source": [
    "print(lat_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 3: 陆地插值函数\n",
    "\n",
    "def interpolate_land(sst_array: xr.DataArray, method: str = 'linear') -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    对SST数据中的NaN值（陆地）进行插值，采用 pandas DataFrame 的插值方法，然后用0填充。\n",
    "    sst_array: 原始SST数据，xarray.DataArray，维度应为 (lat, lon)\n",
    "    method: pandas.DataFrame.interpolate 使用的插值方法 ('linear', 'nearest', 'quadratic', 'cubic', etc.)\n",
    "    返回: 插值后的SST数据 (NaNs 被替换或填充为0)，xarray.DataArray\n",
    "    \"\"\"\n",
    "    # 确保输入是 xarray.DataArray\n",
    "    if not isinstance(sst_array, xr.DataArray):\n",
    "        raise TypeError(\"输入必须是 xarray.DataArray 类型。\")\n",
    "        \n",
    "    sst_values_original = sst_array.values.copy() # 获取numpy数组副本\n",
    "\n",
    "    # 转换为pandas DataFrame\n",
    "    df = pd.DataFrame(sst_values_original)\n",
    "\n",
    "    # 1. 先按列插值 (axis=0)\n",
    "    # limit_direction='both' 会尝试向前和向后填充\n",
    "    df.interpolate(method=method, axis=0, limit_direction='both', inplace=True)\n",
    "    \n",
    "    # 2. 再按行插值 (axis=1)\n",
    "    df.interpolate(method=method, axis=1, limit_direction='both', inplace=True)\n",
    "\n",
    "    # 3. 将剩余的所有NaN值（如果插值未能完全覆盖）填充为0\n",
    "    sst_values_filled = df.fillna(0.0).values\n",
    "\n",
    "    return xr.DataArray(\n",
    "        sst_values_filled.astype(np.float32), \n",
    "        coords=sst_array.coords, \n",
    "        dims=sst_array.dims, \n",
    "        name=sst_array.name + \"_interpolated\" if sst_array.name else \"sst_interpolated\" # 添加后缀\n",
    "    )\n",
    "\n",
    "# --- 演示如何使用 interpolate_land ---\n",
    "print(\"\\n--- 单元格 3 演示 ---\")\n",
    "# 我们尝试使用上一个单元格 (Cell 2) 演示中可能创建的 sst_array_cropped 变量。\n",
    "# 检查 sst_array_cropped 是否存在并且是一个 xarray.DataArray\n",
    "if 'sst_array_cropped' in locals() and isinstance(sst_array_cropped, xr.DataArray):\n",
    "    print(f\"对 'sst_array_cropped' (形状: {sst_array_cropped.shape}) 进行陆地插值...\")\n",
    "    \n",
    "    # 计算插值前的NaN数量\n",
    "    nan_count_before = np.isnan(sst_array_cropped.values).sum()\n",
    "    print(f\"插值前NaN数量: {nan_count_before}\")\n",
    "\n",
    "    if nan_count_before == 0:\n",
    "        print(\"图像中没有NaN值，无需插值。\")\n",
    "        sst_array_interpolated = sst_array_cropped # 直接使用原图\n",
    "    elif nan_count_before == sst_array_cropped.size: # 如果整个图像都是NaN\n",
    "        print(\"警告:整个图像都是NaN，插值结果将全部为0。\")\n",
    "        sst_array_interpolated = interpolate_land(sst_array_cropped, method='linear')\n",
    "    else:\n",
    "        sst_array_interpolated = interpolate_land(sst_array_cropped, method='linear') # 默认用线性插值\n",
    "\n",
    "    # 计算插值后的NaN数量\n",
    "    nan_count_after = np.isnan(sst_array_interpolated.values).sum()\n",
    "    print(f\"插值后NaN数量: {nan_count_after}\")\n",
    "    print(f\"插值后的数据形状: {sst_array_interpolated.shape}\")\n",
    "    print(f\"插值后SST值范围: {float(sst_array_interpolated.min()):.2f} to {float(sst_array_interpolated.max()):.2f}\")\n",
    "\n",
    "    if nan_count_after == 0:\n",
    "        print(\"成功：所有NaN值已通过插值或填充为0处理完毕。\")\n",
    "    else:\n",
    "        print(\"警告：插值后仍然存在NaN值，请检查插值逻辑或原始数据。\")\n",
    "    \n",
    "    # (可选) 可视化一小部分来对比插值前后的效果，但对于大型图像可能不直观\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sst_array_cropped.plot()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sst_array_interpolated.plot()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"警告: 变量 'sst_array_cropped' 未在之前的单元格中成功创建或类型不正确。\")\n",
    "    print(\"无法执行插值演示。请确保单元格2已成功运行并加载了示例文件。\")\n",
    "    # 创建一个假的 sst_array_interpolated 以便后续单元格可能需要它\n",
    "    sst_array_interpolated = xr.DataArray(np.zeros((IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH), dtype=np.float32), # 用0填充\n",
    "                                          dims=('latitude', 'longitude'), name=\"sst_interpolated_dummy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 4: 归一化函数与演示\n",
    "\n",
    "# (必要的导入如 np, xr, torch, os, glob, datetime, timedelta, tqdm 已经在单元格 1 中完成)\n",
    "# (全局配置变量也已在单元格 1 定义)\n",
    "\n",
    "def get_normalization_stats(file_paths_for_stats: list, \n",
    "                            crop_dims: tuple, \n",
    "                            interpolation_method: str = 'linear' # 与 interpolate_land 保持一致\n",
    "                           ) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    计算SST数据的全局最小值和最大值，用于归一化。\n",
    "    仅使用指定文件列表（训练集文件）的数据进行计算。\n",
    "    \"\"\"\n",
    "    print(f\"开始计算归一化统计参数，使用 {len(file_paths_for_stats)} 个训练文件...\")\n",
    "    \n",
    "    all_sst_values_for_stats = [] # 重命名以示区分\n",
    "    \n",
    "    for file_path in tqdm(file_paths_for_stats, desc=\"读取训练文件计算统计参数\"):\n",
    "        try:\n",
    "            sst_array_raw, _, _ = load_single_nc_file(file_path) # 使用单元格2定义的函数\n",
    "            sst_cropped = crop_image(sst_array_raw, crop_dims[0], crop_dims[1]) # 使用单元格2定义的函数\n",
    "            # 注意: interpolate_land 现在使用 pandas，它需要 xarray.DataArray\n",
    "            sst_interpolated = interpolate_land(sst_cropped, method=interpolation_method) # 使用单元格3定义的函数\n",
    "            all_sst_values_for_stats.append(sst_interpolated.values.flatten())\n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {file_path} 时跳过（统计计算阶段）: {e}\")\n",
    "            continue\n",
    "            \n",
    "    if not all_sst_values_for_stats:\n",
    "        raise ValueError(\"未能从任何指定的训练文件中收集到SST数据以计算归一化统计参数。\")\n",
    "\n",
    "    all_sst_values_np = np.concatenate(all_sst_values_for_stats)\n",
    "    min_sst_val = np.nanmin(all_sst_values_np) \n",
    "    max_sst_val = np.nanmax(all_sst_values_np)\n",
    "    \n",
    "    print(f\"\\n归一化统计参数计算完成：Min_SST = {min_sst_val:.4f}, Max_SST = {max_sst_val:.4f}\")\n",
    "    return float(min_sst_val), float(max_sst_val)\n",
    "\n",
    "def normalize_sst(sst_array: xr.DataArray, min_val: float, max_val: float) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    将SST数据归一化到 [-1, 1] 范围。\n",
    "    \"\"\"\n",
    "    if max_val == min_val:\n",
    "        print(\"警告: SST数据的最小值和最大值相同。归一化结果将全部为0。\")\n",
    "        return xr.full_like(sst_array, 0.0, dtype=np.float32)\n",
    "\n",
    "    normalized_values = 2 * (sst_array.values - min_val) / (max_val - min_val) - 1\n",
    "    normalized_values = np.clip(normalized_values, -1.0, 1.0) \n",
    "    \n",
    "    new_name = sst_array.name + \"_normalized\" if sst_array.name else \"sst_normalized\"\n",
    "    return xr.DataArray(\n",
    "        normalized_values.astype(np.float32), \n",
    "        coords=sst_array.coords, \n",
    "        dims=sst_array.dims, \n",
    "        name=new_name\n",
    "    )\n",
    "\n",
    "# --- 演示如何使用这些函数 ---\n",
    "print(\"\\n--- 单元格 4 演示 ---\")\n",
    "\n",
    "# 1. 确定训练集文件列表 (基于 DATA_START_DATE 和 TRAIN_PERIOD_END_DATE)\n",
    "train_files_for_stats = []\n",
    "train_period_start_dt = datetime.strptime(DATA_START_DATE, \"%Y-%m-%d\")\n",
    "train_period_end_dt = datetime.strptime(TRAIN_PERIOD_END_DATE, \"%Y-%m-%d\")\n",
    "\n",
    "current_dt_iter = train_period_start_dt\n",
    "while current_dt_iter <= train_period_end_dt:\n",
    "    date_str_nodash = current_dt_iter.strftime(\"%Y%m%d\")\n",
    "    filename = FILENAME_TEMPLATE.format(date_str=date_str_nodash)\n",
    "    file_path = os.path.join(DATA_RAW_PATH, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        train_files_for_stats.append(file_path)\n",
    "    current_dt_iter += timedelta(days=1)\n",
    "print(f\"训练集文件数:{len(train_files_for_stats)}\")\n",
    "\n",
    "# 2. 计算归一化统计参数\n",
    "ensure_dir(DATA_PROCESSED_PATH) # 确保预处理数据目录存在\n",
    "if not os.path.exists(NORMALIZATION_STATS_PATH):\n",
    "    global_min_sst, global_max_sst = get_normalization_stats(\n",
    "        train_files_for_stats, \n",
    "        crop_dims=(IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH), \n",
    "        interpolation_method='linear' # 使用线性插值\n",
    "    )\n",
    "    torch.save({'min_sst': global_min_sst, 'max_sst': global_max_sst}, NORMALIZATION_STATS_PATH)\n",
    "    print(f\"归一化统计参数已保存到: {NORMALIZATION_STATS_PATH}\")\n",
    "else:\n",
    "    print(f\"归一化统计参数文件已存在: {NORMALIZATION_STATS_PATH}，直接加载...\")\n",
    "    stats = torch.load(NORMALIZATION_STATS_PATH)\n",
    "    global_min_sst = stats['min_sst']\n",
    "    global_max_sst = stats['max_sst']\n",
    "    print(f\"加载的归一化统计参数: Min_SST = {global_min_sst:.4f}, Max_SST = {global_max_sst:.4f}\")\n",
    "\n",
    "# 3. 对示例数据进行归一化\n",
    "print(f\"\\n对插值后的示例数据进行归一化(形状: {sst_array_interpolated.shape})...\")\n",
    "sst_array_normalized = normalize_sst(sst_array_interpolated, global_min_sst, global_max_sst)\n",
    "print(f\"归一化后的数据形状: {sst_array_normalized.shape}\")\n",
    "print(f\"归一化后数据范围: {float(sst_array_normalized.min()):.2f} to {float(sst_array_normalized.max()):.2f}\")\n",
    "# 可视化归一化前后的数据\n",
    "sst_array_normalized.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 5: 创建陆地海洋掩码函数与演示\n",
    "\n",
    "def create_land_sea_mask(raw_sst_array:xr.DataArray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    创建全局海洋陆地掩码，海洋：True，陆地：Fales\n",
    "    raw_sst_array: xarray.DataArray，包含原始SST数据,需要经过裁剪。\n",
    "    \"\"\"\n",
    "    return torch.from_numpy(\n",
    "        (~np.isnan(raw_sst_array.values)).astype(bool)\n",
    "    )\n",
    "\n",
    "print(\"\\n--- 单元格 5 演示 ---\")\n",
    "if sst_array_cropped.shape != (IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH):\n",
    "    raise ValueError(f\"警告: sst_array_cropped 的形状 {sst_array_cropped.shape} 与目标形状 {IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH} 不匹配。\")\n",
    "\n",
    "# 保存陆地海洋掩码\n",
    "ensure_dir(DATA_PROCESSED_PATH) # 确保预处理数据目录存在\n",
    "if not os.path.exists(LAND_SEA_MASK_PATH):\n",
    "    land_sea_mask = create_land_sea_mask(sst_array_cropped)\n",
    "    torch.save(land_sea_mask, LAND_SEA_MASK_PATH)\n",
    "    print(f\"陆地海洋掩码已保存到: {LAND_SEA_MASK_PATH}\")    \n",
    "else:\n",
    "    land_sea_mask = torch.load(LAND_SEA_MASK_PATH)\n",
    "    print(f\"陆地海洋掩码文件已存在: {LAND_SEA_MASK_PATH}，直接加载...\")\n",
    "print(f\"创建的陆地海洋掩码形状: {land_sea_mask.shape}, 数据类型: {land_sea_mask.dtype}\")\n",
    "\n",
    "mask_xr = xr.DataArray(\n",
    "    land_sea_mask.numpy(), \n",
    "    dims=('latitude', 'longitude'), \n",
    "    coords={'latitude': sst_array_cropped.latitude, 'longitude': sst_array_cropped.longitude}\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sst_array_cropped.plot()\n",
    "plt.subplot(1, 2, 2)\n",
    "mask_xr.plot(cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 6: 创建并保存图像块 (Patches)\n",
    "\n",
    "def create_and_save_patches(\n",
    "    daily_sst_normalized_array: xr.DataArray, \n",
    "    date_str: str, # 'YYYY-MM-DD' 格式\n",
    "    patch_size: int, \n",
    "    stride: int, \n",
    "    output_base_dir: str # 例如 config.PATCHES_PATH\n",
    "):\n",
    "    \"\"\"\n",
    "    从单日已归一化的SST图像中提取patches并保存。每日patches保存在一个文件中。包括原始信息和位置信息。\n",
    "    \"\"\"\n",
    "\n",
    "    height,width = daily_sst_normalized_array.shape\n",
    "    sst_np = daily_sst_normalized_array.values\n",
    "    \n",
    "\n",
    "    ensure_dir(output_base_dir)\n",
    "    patches_one_day = [] # 存储单日的所有patches信息\n",
    "    for r in range(0, height - patch_size + 1, stride):\n",
    "        for c in range(0, width - patch_size + 1, stride):\n",
    "            patch_np = sst_np[r:r + patch_size, c:c + patch_size]\n",
    "\n",
    "            patch_data = {\n",
    "                'sst_patch': torch.from_numpy(patch_np.astype(np.float32)),\n",
    "                'coords':(r,c)\n",
    "            }\n",
    "            patches_one_day.append(patch_data)\n",
    "    daily_patches_filename = f\"{date_str}_patches.pt\"\n",
    "    filepath = os.path.join(output_base_dir, daily_patches_filename)\n",
    "    torch.save(patches_one_day, filepath)\n",
    "    print(f\"Saved {len(patches_one_day)} patches for {date_str}\")\n",
    "    return len(patches_one_day)  # 返回单日patches的数量\n",
    "\n",
    "print(\"\\n--- 单元格 6 演示 ---\")\n",
    "\n",
    "ensure_dir(PATCHES_PATH) # 确保patches目录存在\n",
    "sample_date_str = DATA_START_DATE # 使用单元格1定义的全局变量\n",
    "num_pathes_created_one_day = create_and_save_patches(\n",
    "    daily_sst_normalized_array=sst_array_normalized, \n",
    "    date_str=sample_date_str, # 示例日期\n",
    "    patch_size=PATCH_SIZE, \n",
    "    stride=STRIDE, \n",
    "    output_base_dir=PATCHES_PATH\n",
    ")\n",
    "print(f\"为日期 {sample_date_str} 创建了 {num_pathes_created_one_day} 个patches。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 7: run_preprocessing 总控函数\n",
    "def run_prepocess():\n",
    "    \"\"\"\n",
    "    执行完整的预处理流程：\n",
    "    1. 确定文件列表和日期范围。\n",
    "    2. (如果不存在) 计算并保存归一化统计参数 (仅基于训练期数据)。\n",
    "    3. (如果不存在) 创建并保存陆地海洋掩码。\n",
    "    4. 逐日处理数据：加载、裁剪、插值、归一化。\n",
    "    5. 调用 create_and_save_patches 将每日所有patches保存到单个汇总文件中。\n",
    "    \"\"\"\n",
    "    ensure_dir(DATA_PROCESSED_PATH) # 确保预处理数据目录存在\n",
    "    ensure_dir(PATCHES_PATH) # 确保patches目录存在\n",
    "    # 如果有其他结果目录，也可以在这里确保它们存在\n",
    "    ensure_dir(RESULTS_PATH) \n",
    "    ensure_dir(CHECKPOINT_PATH)\n",
    "    ensure_dir(FIGURES_PATH)\n",
    "    ensure_dir(PREDICTIONS_PATH)\n",
    "\n",
    "    # 1. 所有可用文件和对应日期\n",
    "    start_date_dt = datetime.strptime(DATA_START_DATE, \"%Y-%m-%d\")\n",
    "    end_date_dt = datetime.strptime(DATA_END_DATE, \"%Y-%m-%d\")\n",
    "    train_period_end_dt = datetime.strptime(TRAIN_PERIOD_END_DATE, \"%Y-%m-%d\")\n",
    "\n",
    "    all_file_paths = []\n",
    "    date_objects_for_prepocess = []\n",
    "\n",
    "    current_dt_iter = start_date_dt\n",
    "    while current_dt_iter <= end_date_dt:\n",
    "        date_str_nodash = current_dt_iter.strftime(\"%Y%m%d\")\n",
    "        filename = FILENAME_TEMPLATE.format(date_str=date_str_nodash)\n",
    "        file_path = os.path.join(DATA_RAW_PATH, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            all_file_paths.append(file_path)\n",
    "            date_objects_for_prepocess.append(current_dt_iter)\n",
    "        else:\n",
    "            raise ValueError(f\"错误: 文件 {file_path} 不存在。请检查原始数据路径和文件名格式。\")\n",
    "        current_dt_iter += timedelta(days=1)\n",
    "    \n",
    "    print(f\"找到 {len(all_file_paths)} 个可用的原始数据文件。\")\n",
    "    \n",
    "    # 2. 计算归一化统计参数 (仅基于训练期数据)\n",
    "    train_files_for_stats = []\n",
    "    for i,dt_obj in enumerate(date_objects_for_prepocess):\n",
    "        if dt_obj <= train_period_end_dt:\n",
    "            train_files_for_stats.append(all_file_paths[i])\n",
    "\n",
    "    if not train_files_for_stats:\n",
    "        raise ValueError(\"未找到任何训练期数据文件。请检查 DATA_START_DATE 和 TRAIN_PERIOD_END_DATE 设置。\")\n",
    "    \n",
    "    print(f\"使用截止到 {TRAIN_PERIOD_END_DATE} 的 {len(train_files_for_stats)} 个训练文件计算归一化统计参数...\")\n",
    "\n",
    "    if not os.path.exists(NORMALIZATION_STATS_PATH):\n",
    "        min_sst, max_sst = get_normalization_stats(\n",
    "            train_files_for_stats, \n",
    "            crop_dims=(IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH), \n",
    "            interpolation_method='linear'\n",
    "        )\n",
    "        torch.save({'min_sst': min_sst, 'max_sst': max_sst}, NORMALIZATION_STATS_PATH)\n",
    "        print(f\"归一化统计参数已保存到: {NORMALIZATION_STATS_PATH}\")\n",
    "    else:\n",
    "        stats = torch.load(NORMALIZATION_STATS_PATH)\n",
    "        min_sst , max_sst = stats['min_sst'], stats['max_sst']\n",
    "        print(f\"从{NORMALIZATION_STATS_PATH}加载的归一化统计参数: Min_SST = {min_sst:.4f}, Max_SST = {max_sst:.4f}\")\n",
    "\n",
    "    # 3. 创建陆地海洋掩码 (如果不存在)\n",
    "    if not os.path.exists(LAND_SEA_MASK_PATH):\n",
    "        print(f\"使用{all_file_paths[0]}创建陆地海洋掩码...\")\n",
    "        sample_sst_raw,_,_ = load_single_nc_file(all_file_paths[0])\n",
    "        sample_sst_cropped = crop_image(sample_sst_raw, IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH)\n",
    "        land_sea_mask = create_land_sea_mask(sample_sst_cropped)\n",
    "        torch.save(land_sea_mask, LAND_SEA_MASK_PATH)\n",
    "        print(f\"陆地海洋掩码已保存到: {LAND_SEA_MASK_PATH}\")\n",
    "    else:\n",
    "        print(f\"陆地海洋掩码文件已存在: {LAND_SEA_MASK_PATH}\")\n",
    "    \n",
    "    # 4. 逐日处理数据(加载、裁剪、插值、归一化，分割)\n",
    "    print(f\"开始从{DATA_START_DATE}到{DATA_END_DATE}处理每日数据并保存patches...\")\n",
    "    for i , filename in enumerate(tqdm(all_file_paths,desc=\"预处理每日数据\")):\n",
    "        date_obj = date_objects_for_prepocess[i]\n",
    "        date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "        if os.path.exists(os.path.join(PATCHES_PATH, f\"{date_str}_patches.pt\")):\n",
    "            print(f\"跳过 {date_str}，已存在处理后的patches文件。\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # 1. 加载数据\n",
    "            sst_raw, _, _ = load_single_nc_file(filename)\n",
    "\n",
    "            # 2. 裁剪到目标尺寸\n",
    "            sst_cropped = crop_image(sst_raw, IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH)\n",
    "\n",
    "            # 3. 插值陆地海洋掩码\n",
    "            sst_interpolated = interpolate_land(sst_cropped, method='linear')\n",
    "\n",
    "            # 4. 归一化\n",
    "            sst_normalized = normalize_sst(sst_interpolated, min_sst, max_sst)\n",
    "\n",
    "            # 5. 创建并保存patches\n",
    "            num_patches = create_and_save_patches(\n",
    "                daily_sst_normalized_array=sst_normalized, \n",
    "                date_str=date_str, \n",
    "                patch_size=PATCH_SIZE, \n",
    "                stride=STRIDE, \n",
    "                output_base_dir=PATCHES_PATH\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"处理文件 {filename} 时发生错误: {e}\")\n",
    "\n",
    "    print(\"\\n数据预处理流程 (run_preprocessing) 全部完成。\")\n",
    "    print(f\"归一化参数是基于 {DATA_START_DATE} 至 {TRAIN_PERIOD_END_DATE} 的数据计算的。\")\n",
    "    print(f\"所有日期的每日汇总patch文件已处理并保存到 {PATCHES_PATH}。\")\n",
    "\n",
    "# 执行预处理流程\n",
    "run_prepocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PATCHES_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m example_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mPATCHES_PATH\u001b[49m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_START_DATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_patches.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m example_file \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(example_filename)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m示例文件shapes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(example_file)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m patches\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PATCHES_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# 检查一个.pt文件\n",
    "import random\n",
    "import os\n",
    "example_filename = os.path.join(PATCHES_PATH, f\"{DATA_START_DATE}_patches.pt\")\n",
    "example_file = torch.load(example_filename)\n",
    "print(f\"示例文件shapes: {len(example_file)} patches\")\n",
    "print(f\"示例文件第一个patch形状: {example_file[0]['sst_patch'].shape}, 坐标: {example_file[0]['coords']}\")\n",
    "# 随即展示6个patches\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(6):\n",
    "    ex = example_file[random.randint(0,924)]['sst_patch']\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    plt.imshow(ex.numpy(), cmap='coolwarm')\n",
    "    plt.title(f'Patch {i+1}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "# 创建一个统一的colorbar\n",
    "cbar = plt.colorbar(plt.cm.ScalarMappable(cmap='coolwarm'), ax=plt\n",
    ".gcf().get_axes(), orientation='vertical', fraction=0.02, pad=0.04)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 拼接一张完整的图像\n",
    "import numpy as np\n",
    "import torch\n",
    "def stitch_patches_to_image(patches: list, patch_size: int, stride: int, image_shape: tuple) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    将patches拼接回完整图像。\n",
    "    patches: list of dicts, 每个包含 'sst_patch' 和 'coords' 键\n",
    "    patch_size: int, 每个patch的尺寸\n",
    "    stride: int, 每个patch之间的步幅\n",
    "    image_shape: tuple, 完整图像的形状 (height, width)\n",
    "    返回: 拼接后的完整图像，torch.Tensor\n",
    "    \"\"\"\n",
    "    height, width = image_shape\n",
    "    stitched_image = torch.zeros((height, width), dtype=torch.float32)\n",
    "\n",
    "    for patch_info in patches:\n",
    "        patch = patch_info['sst_patch']\n",
    "        r, c = patch_info['coords']\n",
    "        stitched_image[r:r + patch_size, c:c + patch_size] = patch\n",
    "\n",
    "    return stitched_image\n",
    "# 测试拼接函数\n",
    "# complete_img = stitch_patches_to_image(\n",
    "#     example_file, \n",
    "#     patch_size=PATCH_SIZE, \n",
    "#     stride=STRIDE, \n",
    "#     image_shape=(IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH)\n",
    "# )\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# # 反转y轴，因为图像数据通常是从上到下的\n",
    "# plt.imshow(complete_img.numpy(), cmap='coolwarm', origin='lower')\n",
    "# plt.colorbar(fraction=0.02, pad=0.04)\n",
    "# plt.title('Stitched Complete Image')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 8: 特征工程 (时间/空间编码与条件MLP)\n",
    "\n",
    "# 1. time_embedding\n",
    "def get_time_features(date_str: str, reference_year: int = 2020):\n",
    "    \"\"\"\n",
    "    为给定的日期字符串生成时间特征。\n",
    "    包括：归一化的年份、月份和日期的周期性编码（正弦/余弦）。\n",
    "    返回: torch.Tensor: 包含时间特征的一维张量 (5个特征)。\n",
    "    \"\"\"\n",
    "    dt_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    year_norm = (dt_obj.year - reference_year) / 10.0  # 假设2020年为参考年\n",
    "    month_sin = np.sin(2 * np.pi * dt_obj.month / 12.0)\n",
    "    month_cos = np.cos(2 * np.pi * dt_obj.month / 12.0)\n",
    "    days_in_month = calendar.monthrange(dt_obj.year, dt_obj.month)[1]  # 获取当月天数\n",
    "    day_sin = np.sin(2 * np.pi * dt_obj.day / days_in_month)\n",
    "    day_cos = np.cos(2 * np.pi * dt_obj.day / days_in_month)\n",
    "    return torch.tensor([year_norm, month_sin, month_cos, day_sin, day_cos], dtype=torch.float32)\n",
    "\n",
    "# 2.spatail_embedding\n",
    "def get_spatial_features(patch_coords: tuple, full_image_dims: tuple, patch_size: int):\n",
    "    \"\"\"\n",
    "    为给定的patch坐标生成归一化的空间特征 (左上角坐标归一化)。\n",
    "    返回: torch.Tensor: 包含归一化空间特征的一维张量 (2个特征)。\n",
    "    \"\"\"\n",
    "    row, col = patch_coords\n",
    "    height, width = full_image_dims\n",
    "    normalized_row = row / (height - patch_size)\n",
    "    normalized_col = col / (width - patch_size)\n",
    "    return torch.tensor([normalized_row, normalized_col], dtype=torch.float32)\n",
    "\n",
    "# 3. 条件MLP\n",
    "class ConditionalMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    一个MLP，将编码后的时间和空间特征映射为条件嵌入向量 C_cond。\n",
    "    \"\"\"\n",
    "    def __init__(self,num_time_features:int,num_space_features:int,hidden_dims:list,output_dims:int):\n",
    "        super().__init__()\n",
    "        input_dim = num_time_features + num_space_features\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            current_dim = h_dim\n",
    "        layers.append(nn.Linear(current_dim, output_dims))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,time_features:torch.Tensor,spatial_features:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        前向传播：将时间和空间特征拼接后通过MLP。\n",
    "        time_features: torch.Tensor, shape (batch_size, num_time_features)\n",
    "        spatial_features: torch.Tensor, shape (batch_size, num_space_features)\n",
    "        返回: torch.Tensor, shape (batch_size, output_dims)\n",
    "        \"\"\"\n",
    "        combined_features = torch.cat((time_features, spatial_features), dim=1)\n",
    "        return self.mlp(combined_features)\n",
    "    \n",
    "# 实例化MLP\n",
    "def get_conditional_mlp():\n",
    "    \"\"\"根据config中的参数实例化条件MLP\"\"\"\n",
    "    num_time_features = 5  # 5个时间特征\n",
    "    num_sparital_features = 2  # 2个空间特征\n",
    "\n",
    "    return ConditionalMLP(\n",
    "        num_time_features=num_time_features,\n",
    "        num_space_features=num_sparital_features,\n",
    "        hidden_dims=MLP_HIDDEN_DIMS,  # 从配置中获取隐藏层维度\n",
    "        output_dims=CONDITION_EMBED_DIM     # 输出维度\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "    def __init__(self, mode='train'):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.history_days = HISTORY_DAYS\n",
    "        self.train_period_end_dt = datetime.strptime(TRAIN_PERIOD_END_DATE, \"%Y-%m-%d\")\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.start_date_dt = datetime.strptime(DATA_START_DATE, \"%Y-%m-%d\")\n",
    "            self.end_date_dt = self.train_period_end_dt\n",
    "        elif self.mode == 'test':\n",
    "            self.start_date_dt = self.train_period_end_dt + timedelta(days=1)\n",
    "            self.end_date_dt = datetime.strptime(DATA_END_DATE, \"%Y-%m-%d\")\n",
    "        else:\n",
    "            raise ValueError(f\"未知的模式: {self.mode}。mode 请使用 'train' 或 'test'。\")\n",
    "\n",
    "        print(f\"为 '{self.mode}' 模式初始化 SSTDataset，日期范围: {self.start_date_dt.strftime('%Y-%m-%d')} 到 {self.end_date_dt.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        # 加载所有patch到内存\n",
    "        self.all_loaded_patches = defaultdict(dict)  # [date_str][coords] = sst_tensor\n",
    "        print(\"开始加载所有每日patch汇总文件到内存...\")\n",
    "\n",
    "        temp_date = self.start_date_dt\n",
    "        date_range_for_loading = []\n",
    "        while temp_date <= self.end_date_dt:\n",
    "            date_range_for_loading.append(temp_date)\n",
    "            temp_date += timedelta(days=1)\n",
    "\n",
    "        for current_load_dt in tqdm(date_range_for_loading, desc=\"加载每日patches\"):\n",
    "            date_str = current_load_dt.strftime(\"%Y-%m-%d\")\n",
    "            file_path = os.path.join(PATCHES_PATH, f\"{date_str}_patches.pt\")\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    patches = torch.load(file_path)\n",
    "                    for patch in patches:\n",
    "                        coords = tuple(patch['coords'])\n",
    "                        self.all_loaded_patches[date_str][coords] = patch['sst_patch']\n",
    "                except Exception as e:\n",
    "                    print(f\"加载文件 {file_path} 时发生错误: {e}\")\n",
    "            else:\n",
    "                print(f\"警告: 文件 {file_path} 不存在，跳过加载。\")\n",
    "\n",
    "        if not self.all_loaded_patches:\n",
    "            raise ValueError(f\"未能加载任何patches数据，请检查 {PATCHES_PATH} 中的文件是否存在。\")\n",
    "\n",
    "        print(f\"已成功加载 {len(self.all_loaded_patches)} 天的patches数据。\")\n",
    "\n",
    "        # 构造样本序列\n",
    "        self.samples = []\n",
    "        print(\"开始构造样本序列...\")\n",
    "\n",
    "        example_date = next(iter(self.all_loaded_patches))\n",
    "        coords_set = set(self.all_loaded_patches[example_date].keys())\n",
    "        if not coords_set:\n",
    "            raise ValueError(\"未找到任何有效的patch坐标，请检查加载的patches数据。\")\n",
    "        print(f\"所有日期的patches共有 {len(coords_set)} 个唯一坐标。\")\n",
    "\n",
    "        available_dates = sorted(self.all_loaded_patches.keys())\n",
    "\n",
    "        for coords in tqdm(coords_set, desc=f\"为 {self.mode} 构造样本序列\"):\n",
    "            for idx in range(self.history_days, len(available_dates)):\n",
    "                target_date = available_dates[idx]\n",
    "                history_date_strs = [\n",
    "                    (datetime.strptime(target_date, \"%Y-%m-%d\") - timedelta(days=d)).strftime(\"%Y-%m-%d\")\n",
    "                    for d in range(self.history_days, 0, -1)\n",
    "                ]\n",
    "                if all(d in self.all_loaded_patches and coords in self.all_loaded_patches[d] for d in history_date_strs):\n",
    "                    sample = {\n",
    "                        'history_date_strs': history_date_strs,\n",
    "                        'target_date_str': target_date,\n",
    "                        'coords': coords,\n",
    "                    }\n",
    "                    self.samples.append(sample)\n",
    "\n",
    "        print(f\"为 '{self.mode}' 模式成功构造了 {len(self.samples)} 个样本序列。\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        coords = sample['coords']\n",
    "\n",
    "        hist_stack = torch.stack([\n",
    "            self.all_loaded_patches[d][coords] for d in sample['history_date_strs']\n",
    "        ], dim=0)  # shape: [HISTORY_DAYS, 1, patch_h, patch_w]\n",
    "\n",
    "        target_patch = self.all_loaded_patches[sample['target_date_str']][coords]\n",
    "        target = target_patch.unsqueeze(0)  # shape: [1, patch_h, patch_w]\n",
    "\n",
    "        time_feat = get_time_features(\n",
    "            sample['target_date_str'],\n",
    "            reference_year=datetime.strptime(DATA_START_DATE, \"%Y-%m-%d\").year\n",
    "        )\n",
    "        spatial_feat = get_spatial_features(\n",
    "            coords,\n",
    "            full_image_dims=(IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH),\n",
    "            patch_size=PATCH_SIZE\n",
    "        )\n",
    "\n",
    "        return hist_stack, target, time_feat, spatial_feat\n",
    "\n",
    "# print(\"\\n--- 单元格 8 测试 ---\")\n",
    "# train_dataset = SSTDataset(mode='train')\n",
    "# train_dataloader = DataLoader(\n",
    "#     train_dataset, \n",
    "#     batch_size=BATCH_SIZE, \n",
    "#     shuffle=True, \n",
    "#     pin_memory=True\n",
    "# )\n",
    "# for hist_stack, target, time_feat, spatial_feat in train_dataloader:\n",
    "#     print(f\"历史堆栈形状: {hist_stack.shape}\\n 目标形状: {target.shape}\\n 时间特征形状: {time_feat.shape}\\n 空间特征形状: {spatial_feat.shape}\")\n",
    "#     print(f\"时间特征示例: {time_feat[0]}\")\n",
    "#     print(f\"空间特征示例: {spatial_feat[0]}\")\n",
    "#     break  # 只查看第一个batch的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 10: 模型定义 (DiffusionUNetWithMLP)\n",
    "\n",
    "from diffusers import UNet2DModel # 确保导入\n",
    "\n",
    "class DiffusionUNetWithMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    封装了条件MLP和U-Net的扩散模型。\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. 初始化条件MLP (从单元格8定义的函数获取)\n",
    "        self.conditional_mlp = get_conditional_mlp() \n",
    "\n",
    "        # 2. 初始化U-Net (UNet2DModel)\n",
    "        # 参数从单元格1定义的全局配置中读取\n",
    "        self.unet = UNet2DModel(\n",
    "            sample_size=(PATCH_SIZE, PATCH_SIZE), \n",
    "            in_channels=UNET_IN_CHANNELS,        \n",
    "            out_channels=UNET_OUT_CHANNELS,      \n",
    "            block_out_channels=UNET_BLOCK_OUT_CHANNELS,\n",
    "            down_block_types=UNET_DOWN_BLOCK_TYPES,\n",
    "            up_block_types=UNET_UP_BLOCK_TYPES,\n",
    "            class_embed_type=UNET_CLASS_EMBED_TYPE,\n",
    "            num_class_embeds=UNET_NUM_CLASS_EMBEDS    \n",
    "            # 根据需要可以添加其他 UNet2DModel 支持的参数，例如:\n",
    "            # norm_num_groups=32, \n",
    "            # dropout=0.0\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                history_patches: torch.Tensor,    # (B, HISTORY_DAYS, H, W)\n",
    "                noisy_target_patch: torch.Tensor, # (B, 1, H, W) - 带噪声的目标patch\n",
    "                timestep: torch.Tensor,           # (B,) - 扩散时间步\n",
    "                raw_time_features: torch.Tensor,  # (B, num_raw_time_features)\n",
    "                raw_spatial_features: torch.Tensor # (B, num_raw_space_features)\n",
    "               ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        模型的前向传播。\n",
    "        返回: U-Net预测的噪声，形状 (B, UNET_OUT_CHANNELS, H, W)\n",
    "        \"\"\"\n",
    "        # 1. 通过MLP生成条件嵌入 C_cond\n",
    "        condition_embedding_C_cond = self.conditional_mlp(raw_time_features, raw_spatial_features)\n",
    "\n",
    "        # 2. 准备U-Net的输入图像数据\n",
    "        unet_input = torch.cat((history_patches, noisy_target_patch), dim=1) # (B, 31, H, W)\n",
    "\n",
    "        # 3. U-Net前向传播\n",
    "        # 当 class_embed_type=\"identity\", class_labels 参数接收的是实际的嵌入向量\n",
    "        predicted_noise = self.unet(\n",
    "            sample=unet_input,\n",
    "            timestep=timestep,\n",
    "            class_labels=condition_embedding_C_cond \n",
    "        ).sample \n",
    "\n",
    "        return predicted_noise\n",
    "\n",
    "# --- 辅助函数：根据config实例化并返回主模型 ---\n",
    "def get_diffusion_model():\n",
    "    \"\"\"\n",
    "    根据 config 中的 MODEL_ARCHITECTURE 创建并返回主模型实例。\n",
    "    \"\"\"\n",
    "    # MODEL_ARCHITECTURE 来自单元格1的全局配置\n",
    "    if MODEL_ARCHITECTURE == \"classic_unet\":\n",
    "        print(f\"初始化模型: DiffusionUNetWithMLP (基于 UNet2DModel)\")\n",
    "        return DiffusionUNetWithMLP() # 重命名变量以防混淆\n",
    "    else:\n",
    "        raise ValueError(f\"未知的模型架构在config中定义: {MODEL_ARCHITECTURE}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- 单元格 10 演示 (可选) ---\")\n",
    "try:\n",
    "    # 尝试实例化模型\n",
    "    print(\"尝试实例化 DiffusionUNetWithMLP...\")\n",
    "    demo_model = get_diffusion_model()\n",
    "    print(\"模型实例化成功。\")\n",
    "    print(demo_model) # 可以打印模型结构\n",
    "    \n",
    "    # 准备一些假的输入数据进行一次前向传播测试 (确保形状正确)\n",
    "    print(\"\\n尝试一次假的前向传播...\")\n",
    "    dummy_batch_size = 2\n",
    "    dummy_history = torch.randn(dummy_batch_size, HISTORY_DAYS, PATCH_SIZE, PATCH_SIZE)\n",
    "    dummy_noisy_target = torch.randn(dummy_batch_size, TARGET_DAYS, PATCH_SIZE, PATCH_SIZE)\n",
    "    dummy_timestep = torch.randint(0, DDPM_NUM_TRAIN_TIMESTEPS, (dummy_batch_size,)).long()\n",
    "    \n",
    "    # 假设原始时间特征有5个，空间特征有2个 (与features.py中的定义一致)\n",
    "    num_raw_time_feat = 5\n",
    "    num_raw_space_feat = 2\n",
    "    dummy_raw_time = torch.randn(dummy_batch_size, num_raw_time_feat)\n",
    "    dummy_raw_space = torch.randn(dummy_batch_size, num_raw_space_feat)\n",
    "    \n",
    "    # 将模型和数据移到设备\n",
    "    demo_model.to(DEVICE)\n",
    "    dummy_history = dummy_history.to(DEVICE)\n",
    "    dummy_noisy_target = dummy_noisy_target.to(DEVICE)\n",
    "    dummy_timestep = dummy_timestep.to(DEVICE)\n",
    "    dummy_raw_time = dummy_raw_time.to(DEVICE)\n",
    "    dummy_raw_space = dummy_raw_space.to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad(): # 测试时不需要梯度\n",
    "        prediction = demo_model(dummy_history, dummy_noisy_target, dummy_timestep, dummy_raw_time, dummy_raw_space)\n",
    "    print(f\"假的前向传播输出形状: {prediction.shape}\") # 应为 (dummy_batch_size, UNET_OUT_CHANNELS, PATCH_SIZE, PATCH_SIZE)\n",
    "    if prediction.shape == (dummy_batch_size, UNET_OUT_CHANNELS, PATCH_SIZE, PATCH_SIZE):\n",
    "        print(\"前向传播形状符合预期。\")\n",
    "    else:\n",
    "        print(\"错误：前向传播形状不符合预期！\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"单元格 10 演示部分发生错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 11: 训练流程\n",
    "\n",
    "# (必要的导入如 torch, F, DataLoader, DDPMScheduler, get_lr_scheduler, AdamW, \n",
    "#  tqdm, plt, os, np 已经在单元格 1 中完成或隐式可用)\n",
    "# (全局配置变量如 BATCH_SIZE, LEARNING_RATE, NUM_EPOCHS, DEVICE, etc. 已在单元格 1 定义)\n",
    "# (SSTDataset 类已在单元格 9 定义)\n",
    "# (get_diffusion_model 函数已在单元格 10 定义)\n",
    "# (ensure_dir 函数已在单元格 2 定义)\n",
    "\n",
    "from torch.utils.data import DataLoader # 确保显式导入\n",
    "from diffusers import DDPMScheduler\n",
    "from diffusers.optimization import get_scheduler as get_lr_scheduler\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "# --- 辅助函数 ---\n",
    "def save_checkpoint(epoch, model, optimizer, loss, filepath):\n",
    "    \"\"\"保存模型checkpoint\"\"\"\n",
    "    # ensure_dir 来自单元格 2\n",
    "    ensure_dir(os.path.dirname(filepath))\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss, # 保存当前epoch的平均loss\n",
    "    }, filepath)\n",
    "    print(f\"Checkpoint 已保存至: {filepath} (Epoch {epoch}, Loss {loss:.4f})\")\n",
    "\n",
    "def plot_loss_curve(epoch_losses_list, filepath): # 参数名修改以示列表\n",
    "    \"\"\"绘制并保存loss曲线图\"\"\"\n",
    "    # ensure_dir 来自单元格 2\n",
    "    ensure_dir(os.path.dirname(filepath))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epoch_losses_list, label='Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Avg Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(filepath)\n",
    "    plt.show() \n",
    "    print(f\"损失曲线图已保存至: {filepath}\")\n",
    "\n",
    "# --- 开始训练的主体代码 ---\n",
    "print(\"开始训练流程的主体代码...\")\n",
    "\n",
    "# 0. 再次确保输出目录存在 (config中已定义路径，ensure_dir在单元格2定义)\n",
    "print(\"确保输出目录存在...\")\n",
    "ensure_dir(CHECKPOINT_PATH)\n",
    "ensure_dir(FIGURES_PATH)\n",
    "\n",
    "# 1. 准备数据 (SSTDataset 在单元格9定义)\n",
    "print(\"初始化训练数据集...\")\n",
    "train_dataset = SSTDataset(mode='train') \n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE, # 来自单元格1的配置\n",
    "    shuffle=True,\n",
    "    num_workers=0, # 在Windows或某些Notebook环境中, num_workers>0 可能导致问题, 先设为0测试\n",
    "                   # 如果在Linux上并且没有问题,可以尝试设置为比如 os.cpu_count() // 2\n",
    "    pin_memory=True if DEVICE == \"cuda\" else False \n",
    ")\n",
    "print(f\"训练数据加载器准备完毕，每个epoch包含 {len(train_dataloader)} 个批次。\")\n",
    "\n",
    "# 2. 初始化模型 (get_diffusion_model 在单元格10定义)\n",
    "print(\"初始化模型...\")\n",
    "model = get_diffusion_model() \n",
    "print(f\"模型参数总数: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "model.to(DEVICE)\n",
    "print(f\"模型已移动到设备: {DEVICE}\")\n",
    "\n",
    "# 3. 初始化 DDPM 噪声调度器\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=DDPM_NUM_TRAIN_TIMESTEPS, # 来自单元格1\n",
    "    beta_schedule=DDPM_BETA_SCHEDULE          # 来自单元格1\n",
    ")\n",
    "print(f\"DDPM噪声调度器 ({DDPM_BETA_SCHEDULE} schedule, {DDPM_NUM_TRAIN_TIMESTEPS} 步) 已初始化。\")\n",
    "\n",
    "# 4. 初始化优化器\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE) # LEARNING_RATE 来自单元格1\n",
    "print(f\"优化器 AdamW 已初始化，学习率: {LEARNING_RATE}\")\n",
    "\n",
    "# 5. 初始化学习率调度器\n",
    "num_training_steps_per_epoch = len(train_dataloader)\n",
    "num_total_training_steps = num_training_steps_per_epoch * NUM_EPOCHS # NUM_EPOCHS 来自单元格1\n",
    "\n",
    "lr_scheduler = get_lr_scheduler(\n",
    "    name=\"linear\", \n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_total_training_steps\n",
    ")\n",
    "print(f\"学习率调度器 (linear) 已初始化，总训练步数: {num_total_training_steps}\")\n",
    "\n",
    "# 6. 训练循环\n",
    "print(f\"开始训练，共 {NUM_EPOCHS} 轮。\")\n",
    "epoch_losses_history = [] # 用于存储每个epoch的平均损失\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train() \n",
    "    running_loss_this_epoch = 0.0 # 重命名以清晰表示是当前epoch的累计损失\n",
    "    \n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_dataloader), \n",
    "        total=len(train_dataloader), \n",
    "        desc=f\"轮次 {epoch+1}/{NUM_EPOCHS}\"\n",
    "    )\n",
    "\n",
    "    for step, batch in progress_bar:\n",
    "        history_patches, target_patch, raw_time_features, raw_spatial_features = batch\n",
    "        \n",
    "        history_patches = history_patches.to(DEVICE)     \n",
    "        target_patch = target_patch.to(DEVICE)          \n",
    "        raw_time_features = raw_time_features.to(DEVICE) \n",
    "        raw_spatial_features = raw_spatial_features.to(DEVICE) \n",
    "        \n",
    "\n",
    "        noise = torch.randn_like(target_patch)\n",
    "        timesteps = torch.randint(\n",
    "            0, \n",
    "            noise_scheduler.config.num_train_timesteps, \n",
    "            (BATCH_SIZE,), \n",
    "            device=DEVICE \n",
    "        ).long()\n",
    "        noisy_target_patch = noise_scheduler.add_noise(target_patch, noise, timesteps)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predicted_noise = model(\n",
    "            history_patches, \n",
    "            noisy_target_patch, \n",
    "            timesteps, \n",
    "            raw_time_features, \n",
    "            raw_spatial_features\n",
    "        )\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        running_loss_this_epoch += loss.item()\n",
    "        progress_bar.set_postfix({\n",
    "            \"损失\": f\"{loss.item():.4f}\", \n",
    "            \"学习率\": f\"{lr_scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "\n",
    "        if step % LOG_INTERVAL == 0 and step > 0: # LOG_INTERVAL 来自单元格1\n",
    "             avg_loss_so_far = running_loss_this_epoch / (step + 1)\n",
    "             print(f\"    [轮次 {epoch+1}, 批次 {step}/{len(train_dataloader)}] 平均批次损失: {avg_loss_so_far:.4f}\")\n",
    "\n",
    "    avg_epoch_loss_value = running_loss_this_epoch / len(train_dataloader)\n",
    "    epoch_losses_history.append(avg_epoch_loss_value)\n",
    "    print(f\"轮次 {epoch+1} 完成。平均训练损失: {avg_epoch_loss_value:.4f}, 当前学习率: {lr_scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "    if (epoch + 1) % SAVE_EPOCH_INTERVAL == 0 or (epoch + 1) == NUM_EPOCHS: # SAVE_EPOCH_INTERVAL 来自单元格1\n",
    "        checkpoint_filepath = os.path.join(CHECKPOINT_PATH, f\"model_epoch_{epoch+1}.pt\")\n",
    "        save_checkpoint(epoch + 1, model, optimizer, avg_epoch_loss_value, checkpoint_filepath)\n",
    "        \n",
    "        loss_curve_filepath = os.path.join(FIGURES_PATH, f\"training_loss_curve_epoch_{epoch+1}.png\")\n",
    "        plot_loss_curve(epoch_losses_history, loss_curve_filepath)\n",
    "\n",
    "print(\"\\n训练完成。\")\n",
    "# 最终保存一次模型和loss图 (确保最后的状态被保存)\n",
    "final_model_path = os.path.join(CHECKPOINT_PATH, \"model_final.pt\")\n",
    "final_loss_value = epoch_losses_history[-1] if epoch_losses_history else float('inf')\n",
    "save_checkpoint(NUM_EPOCHS, model, optimizer, final_loss_value, final_model_path)\n",
    "\n",
    "final_loss_curve_path = os.path.join(FIGURES_PATH, \"training_loss_curve_final.png\")\n",
    "plot_loss_curve(epoch_losses_history, final_loss_curve_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 12: 推理与评估的辅助函数\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def load_checkpoint(filepath,model,device):\n",
    "    \"\"\"加载模型checkpoint\"\"\"\n",
    "    checkpoint = torch.load(filepath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Checkpoint 已加载: {filepath} (Epoch {checkpoint['epoch']}, Loss {checkpoint['loss']:.4f})\")\n",
    "    return model\n",
    "\n",
    "def denormalize_sst(normalized_sst_tensor:torch.Tensor,min_val:float,max_val:float) -> torch.Tensor:\n",
    "    \"\"\"将数据反归一化到原始范围\"\"\"\n",
    "    if max_val == min_val:\n",
    "        raise ValueError(\"max_val 不能等于 min_val\")\n",
    "    original_sst =  (normalized_sst_tensor + 1.0) * (max_val - min_val) / 2.0 + min_val\n",
    "    return original_sst\n",
    "\n",
    "def reconstructure_img_from_pathes(\n",
    "    patches:dict, # key:(r,c) tuple, value: torch.Tensor\n",
    "    img_dims:tuple, # (height, width)\n",
    "    patch_size:int,\n",
    "    stride:int\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"从patch图像重构原始图像,重叠区域取平均\"\"\"\n",
    "    H, W = img_dims\n",
    "    re_img = torch.zeros((H, W), dtype=torch.float32)\n",
    "    count_map = torch.zeros((H, W), dtype=torch.float32)\n",
    "\n",
    "    for (r,c),patch_tensor in patches.items():\n",
    "        re_img[r:r + patch_size, c:c + patch_size] += patch_tensor\n",
    "        count_map[r:r + patch_size, c:c + patch_size] += 1\n",
    "    \n",
    "    re_img = torch.where(count_map > 0, re_img / count_map, 0.0) # 计算平均值，避免除以0\n",
    "\n",
    "    return re_img\n",
    "\n",
    "def calculate_metrics(predicted_np:np.ndarray, target_np:np.ndarray,land_sea_mask_np:np.ndarray):\n",
    "    \"\"\"计算predict和target的MSE,RMSE\"\"\"\n",
    "\n",
    "    # 选择海洋点\n",
    "    ocean_pred = predicted_np[land_sea_mask_np]\n",
    "    ocean_target = target_np[land_sea_mask_np]\n",
    "\n",
    "    mse = mean_squared_error(ocean_target, ocean_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return mse, rmse\n",
    "\n",
    "def save_img_comparison(\n",
    "        predicted_np: np.ndarray,\n",
    "        target_np: np.ndarray,\n",
    "        land_sea_mask_np: np.ndarray,\n",
    "        filepath: str,\n",
    "        title_prefix: str = \"\"\n",
    "):\n",
    "    \"\"\"保存预测、目标与差值图像（带色条）\"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def ensure_dir(directory: str):\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "    ensure_dir(os.path.dirname(filepath))\n",
    "    pred_masked = np.where(land_sea_mask_np, predicted_np, np.nan)\n",
    "\n",
    "    if target_np is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "        im = ax.imshow(pred_masked, cmap='coolwarm')\n",
    "        ax.set_title(f\"{title_prefix} Predicted SST\")\n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)  \n",
    "    else:\n",
    "        target_masked = np.where(land_sea_mask_np, target_np, np.nan)\n",
    "        diff_masked = np.where(land_sea_mask_np, predicted_np - target_np, np.nan)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        im0 = axes[0].imshow(pred_masked, cmap='coolwarm')\n",
    "        axes[0].set_title(f\"{title_prefix} Predicted SST\")\n",
    "        plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "        im1 = axes[1].imshow(target_masked, cmap='coolwarm')\n",
    "        axes[1].set_title(f\"{title_prefix} Target SST\")\n",
    "        plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "        im2 = axes[2].imshow(diff_masked, cmap='RdBu_r')\n",
    "        axes[2].set_title(f\"{title_prefix} Difference (Predicted - Target)\")\n",
    "        plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath)\n",
    "    plt.show()\n",
    "    print(f\"图像对比图已保存到: {filepath}\")\n",
    "\n",
    "print(\"单元格 12: 推理与评估的辅助函数已定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格 13: 推理与评估主体流程\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "# --- 在这里配置本次推理 ---\n",
    "# 可以在运行前修改这两个变量\n",
    "checkpoint_to_load = \"model_final.pt\" \n",
    "# 设置为 None 会自动从测试集第一天开始预测, 或指定一个 'YYYY-MM-DD' 格式的日期\n",
    "inference_start_date_str = \"2020-03-01\" \n",
    "# -------------------------\n",
    "\n",
    "# --- 主体逻辑开始 ---\n",
    "print(\"=\"*50)\n",
    "print(\"开始推理与评估流程...\")\n",
    "print(\"=\"*50+'\\n')\n",
    "\n",
    "device = torch.device(DEVICE)  \n",
    "\n",
    "# 1.基本设置\n",
    "stats = torch.load(NORMALIZATION_STATS_PATH)\n",
    "min_sst, max_sst = stats['min_sst'], stats['max_sst']\n",
    "land_sea_mask = torch.load(LAND_SEA_MASK_PATH)\n",
    "land_sea_mask_np = land_sea_mask.numpy()  # 转换为numpy数组以便后续处理\n",
    "\n",
    "print(f\"归一化参数： Min_SST = {min_sst:.4f}, Max_SST = {max_sst:.4f}\")\n",
    "\n",
    "model = get_diffusion_model()\n",
    "model.to(device)  # 确保模型在正确的设备上\n",
    "model = load_checkpoint(os.path.join(CHECKPOINT_PATH, checkpoint_to_load),model,device)  # 加载模型checkpoint\n",
    "model.eval()  \n",
    "\n",
    "noise_scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=DDPM_NUM_TRAIN_TIMESTEPS, \n",
    "    beta_schedule=DDPM_BETA_SCHEDULE\n",
    ")\n",
    "\n",
    "noise_scheduler.set_timesteps(DDPM_NUM_INFERENCE_STEPS)  # 设置推理步数\n",
    "print(f\"DDPM噪声调度器已设置为 {DDPM_NUM_INFERENCE_STEPS} 步推理。\")\n",
    "\n",
    "# 2. 准备测试数据集\n",
    "if inference_start_date_str is None:\n",
    "    train_end_dt = datetime.strptime(TRAIN_PERIOD_END_DATE, \"%Y-%m-%d\")\n",
    "    inference_start_date_dt = train_end_dt + timedelta(days=1)\n",
    "    inference_start_date_str = inference_start_date_dt.strftime(\"%Y-%m-%d\")\n",
    "else:\n",
    "    inference_start_date_dt = datetime.strptime(inference_start_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "print(f\"将从日期 {inference_start_date_str} 开始进行 {AUTOREGRESSIVE_PREDICT_DAYS} 天的自回归预测。\")\n",
    "print(\"加载初始历史数据...\")\n",
    "current_history_all_coords = defaultdict(list)\n",
    "all_spatial_coords = set()\n",
    "history_start_dt = inference_start_date_dt - timedelta(days=HISTORY_DAYS)\n",
    "\n",
    "date_range_for_history = []\n",
    "temp_date = history_start_dt\n",
    "while temp_date < inference_start_date_dt:\n",
    "    date_range_for_history.append(temp_date)\n",
    "    temp_date += timedelta(days=1)\n",
    "\n",
    "for current_dt in tqdm(date_range_for_history, desc=\"加载历史数据\"):\n",
    "    date_str = current_dt.strftime(\"%Y-%m-%d\")\n",
    "    file_path = os.path.join(PATCHES_PATH, f\"{date_str}_patches.pt\")\n",
    "    if os.path.exists(file_path):\n",
    "        patches = torch.load(file_path, map_location=device)\n",
    "        for patch in patches:\n",
    "            coords = tuple(patch['coords'])\n",
    "            all_spatial_coords.add(coords)\n",
    "            current_history_all_coords[coords].append(patch['sst_patch'])\n",
    "    else:\n",
    "        print(f\"警告: 文件 {file_path} 不存在，跳过加载。\")\n",
    "\n",
    "valid_coords = [coords for coords in all_spatial_coords if len(current_history_all_coords[coords]) == HISTORY_DAYS]\n",
    "if len(valid_coords) < len(all_spatial_coords):\n",
    "    print(f\"警告: 并非所有坐标都有完整的30天历史。将只对 {len(valid_coords)} 个坐标进行预测。\")\n",
    "if not valid_coords:\n",
    "    raise ValueError(\"错误: 没有可用于预测的完整初始历史数据。推理中止。\")\n",
    "\n",
    "ref_year = datetime.strptime(DATA_START_DATE, \"%Y-%m-%d\").year\n",
    "evaluation_metrics = {f\"Tplus{i+1}\":[] for i in range(AUTOREGRESSIVE_PREDICT_DAYS)}\n",
    "\n",
    "# 3. 推理循环\n",
    "with torch.no_grad():\n",
    "    for day in range(AUTOREGRESSIVE_PREDICT_DAYS):\n",
    "        target_pred_dt = inference_start_date_dt + timedelta(days=day)\n",
    "        target_date_str = target_pred_dt.strftime(\"%Y-%m-%d\")\n",
    "        lead_time_key = f\"Tplus{day+1}\"  # 用于存储评估结果的键\n",
    "        print(f\"\\n正在预测 {target_date_str} 的数据...\")\n",
    "\n",
    "        predicted_patches = {}\n",
    "\n",
    "        for coords in tqdm(list(valid_coords), desc=f\"预测Patches:{target_date_str}\"):\n",
    "            # 准备历史数据\n",
    "            history_sequence_list = current_history_all_coords[coords]\n",
    "            history_tensor = torch.stack(history_sequence_list).unsqueeze(0).to(device)  # (1, HISTORY_DAYS, H, W)\n",
    "\n",
    "            # time and spatial features\n",
    "            time_feat = get_time_features(target_date_str, reference_year=ref_year).unsqueeze(0).to(device)\n",
    "            spatial_feat = get_spatial_features(coords, (IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH), PATCH_SIZE).unsqueeze(0).to(device)\n",
    "\n",
    "            noisy_patch_sample = torch.randn(1,UNET_OUT_CHANNELS,PATCH_SIZE,PATCH_SIZE, device=device)  # (1, C, H, W)\n",
    "\n",
    "            for t in noise_scheduler.timesteps:\n",
    "                timestep_tensor = torch.tensor([t], device=device).long()  # 转换为tensor并移动到设备\n",
    "                model_output_noise = model(history_tensor, noisy_patch_sample, timestep_tensor, time_feat, spatial_feat)\n",
    "                noisy_patch_sample = noise_scheduler.step(model_output_noise,t,noisy_patch_sample).prev_sample\n",
    "\n",
    "            pred_path_normalized = noisy_patch_sample.squeeze(0).squeeze(0) \n",
    "            predicted_patches[coords] = pred_path_normalized\n",
    "\n",
    "            # 自回归使用新的预测结果更新历史\n",
    "            if day < AUTOREGRESSIVE_PREDICT_DAYS - 1:\n",
    "                new_history = history_sequence_list[1:] + [pred_path_normalized.detach().clone()]\n",
    "                current_history_all_coords[coords] = new_history\n",
    "        \n",
    "        # 重构预测图像\n",
    "        print(f\"重建图像...\")\n",
    "        full_pred_img_norm = reconstructure_img_from_pathes(\n",
    "            predicted_patches,\n",
    "            img_dims=(IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH),\n",
    "            patch_size=PATCH_SIZE,\n",
    "            stride=STRIDE\n",
    "        )\n",
    "        full_pred_img_denorm = denormalize_sst(full_pred_img_norm, min_sst, max_sst).numpy()\n",
    "        target_img_denorm_np = None\n",
    "        target_img_filename = FILENAME_TEMPLATE.format(date_str=target_pred_dt.strftime(\"%Y-%m-%d\"))\n",
    "        target_img_path = os.path.join(DATA_RAW_PATH, target_img_filename)\n",
    "        if os.path.exists(target_img_path):\n",
    "            target_sst_raw,_,_ = load_single_nc_file(target_img_path)\n",
    "            target_cropped = crop_image(target_sst_raw, IMAGE_TARGET_HEIGHT, IMAGE_TARGET_WIDTH)\n",
    "            target_img_denorm_np = target_cropped.values\n",
    "        \n",
    "            # 计算评估指标\n",
    "            mse,rmse = calculate_metrics(full_pred_img_denorm, target_img_denorm_np, land_sea_mask_np)\n",
    "            print(f\"  评估结果 ({lead_time_key}, {target_date_str}): RMSE = {rmse:.4f}, MSE = {mse:.4f}\")\n",
    "            evaluation_metrics[lead_time_key].append({'date': target_date_str, 'rmse': rmse, 'mse': mse})\n",
    "        else:\n",
    "            print(f\"警告: 目标图像文件 {target_img_path} 不存在，无法计算评估指标。\")\n",
    "\n",
    "# 5. 打印最终的评估指标总结\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- 最终评估指标总结 (针对此次推理序列) ---\")\n",
    "print(\"=\"*50)\n",
    "for lead_time, metrics_list in evaluation_metrics.items():\n",
    "    if not metrics_list:\n",
    "        print(f\"{lead_time}: 无评估数据\")\n",
    "        continue\n",
    "    avg_rmse = np.nanmean([m['rmse'] for m in metrics_list])\n",
    "    avg_mse = np.nanmean([m['mse'] for m in metrics_list])\n",
    "    print(f\"{lead_time} - 平均 RMSE: {avg_rmse:.4f}, 平均 MSE: {avg_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SSTEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
